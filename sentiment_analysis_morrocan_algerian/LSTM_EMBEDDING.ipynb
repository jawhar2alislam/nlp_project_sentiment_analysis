{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2H6EB_yTwmS"
      },
      "outputs": [],
      "source": [
        "!pip install emoji\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "import emoji\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from sklearn.utils.class_weight import compute_class_weight\n"
      ],
      "metadata": {
        "id": "r7-Aj28yT4jt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_tweet(text):\n",
        "    text = re.sub(r'@[A-Za-z0-9_]+', '', str(text))\n",
        "    text = re.sub(r'https?://\\S+', '', text)\n",
        "    text = text.lower().strip()\n",
        "    return emoji.demojize(text, delimiters=(\" \", \" \"))\n"
      ],
      "metadata": {
        "id": "YPmNdmotT6DE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('/content/train.csv')\n",
        "test_df = pd.read_csv('/content/test.csv')\n",
        "val_df = pd.read_csv('/content/validation.csv')\n"
      ],
      "metadata": {
        "id": "kGYvaonMT8VG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for df in [train_df, test_df, val_df]:\n",
        "    df['label'] = df['label'].astype(int)\n",
        "    df['clean_tweet'] = df['tweet'].apply(clean_tweet)\n"
      ],
      "metadata": {
        "id": "ZK8lBi9cT-Ix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_text = train_df['clean_tweet']\n",
        "y_train = train_df['label']\n",
        "X_test_text = test_df['clean_tweet']\n",
        "y_test = test_df['label']\n",
        "X_val_text = val_df['clean_tweet']\n",
        "y_val = val_df['label']\n"
      ],
      "metadata": {
        "id": "q556vtjVT_qL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_words = 5000\n",
        "max_len = 50\n",
        "tokenizer = Tokenizer(num_words=max_words, lower=True)\n",
        "tokenizer.fit_on_texts(X_train_text)\n",
        "\n",
        "X_train_pad = pad_sequences(tokenizer.texts_to_sequences(X_train_text), maxlen=max_len)\n",
        "X_test_pad = pad_sequences(tokenizer.texts_to_sequences(X_test_text), maxlen=max_len)\n",
        "X_val_pad = pad_sequences(tokenizer.texts_to_sequences(X_val_text), maxlen=max_len)\n"
      ],
      "metadata": {
        "id": "M0LIzBZNUBkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "weights_dict = dict(zip(np.unique(y_train), class_weights))\n"
      ],
      "metadata": {
        "id": "-S-_g4yIUDY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 100\n",
        "model = Sequential([\n",
        "    Embedding(max_words, embedding_dim, input_length=max_len),\n",
        "    LSTM(128, return_sequences=False, kernel_regularizer=l2(0.01)),\n",
        "    Dropout(0.5),\n",
        "    Dense(64, activation='relu', kernel_regularizer=l2(0.01)),\n",
        "    Dropout(0.5),\n",
        "    Dense(3, activation='softmax')\n",
        "])\n"
      ],
      "metadata": {
        "id": "9kQKAtpkUFIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n"
      ],
      "metadata": {
        "id": "E5a1lpJOUHE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    X_train_pad, y_train,\n",
        "    epochs=20,\n",
        "    batch_size=32,\n",
        "    validation_data=(X_val_pad, y_val),\n",
        "    class_weight=weights_dict,\n",
        "    callbacks=[lr_scheduler, early_stopping],\n",
        "    verbose=2\n",
        ")\n"
      ],
      "metadata": {
        "id": "EnQCSpqAUI-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_pred = np.argmax(model.predict(X_train_pad), axis=1)\n",
        "y_test_pred = np.argmax(model.predict(X_test_pad), axis=1)\n",
        "y_val_pred = np.argmax(model.predict(X_val_pad), axis=1)\n",
        "\n",
        "print(f\"Train: {accuracy_score(y_train, y_train_pred):.4f}\")\n",
        "print(f\"Test : {accuracy_score(y_test, y_test_pred):.4f}\")\n",
        "print(f\"Val  : {accuracy_score(y_val, y_val_pred):.4f}\")\n"
      ],
      "metadata": {
        "id": "mY9jkE4ZULZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_names = ['positive', 'neutral', 'negative']\n",
        "print(classification_report(y_test, y_test_pred, target_names=target_names))\n"
      ],
      "metadata": {
        "id": "81aqXsTvUNkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(y_test, y_test_pred, labels=[0, 1, 2])\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=target_names,\n",
        "            yticklabels=target_names)\n",
        "plt.title('Matrice de confusion - Test set')\n",
        "plt.xlabel('Prédit')\n",
        "plt.ylabel('Réel')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "RRL-cA5UUQwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
        "plt.title('Accuracy over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "plt.title('Loss over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "k6sP8fVaUVXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inv_label = {0: 'positive', 1: 'neutral', 2: 'negative'}\n",
        "for idx in np.random.choice(len(test_df), 5, replace=False):\n",
        "    print(\"Tweet :\", test_df['tweet'].iloc[idx])\n",
        "    print(\"Vrai label :\", inv_label[y_test.iloc[idx]], \"| Prédit :\", inv_label[y_test_pred[idx]])\n",
        "    print(\"-\" * 50)\n"
      ],
      "metadata": {
        "id": "Fsqs03oGUYzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_no_dropout = Sequential([\n",
        "    Embedding(max_words, embedding_dim, input_length=max_len),\n",
        "    LSTM(128, return_sequences=False, kernel_regularizer=l2(0.01)),\n",
        "    Dense(64, activation='relu', kernel_regularizer=l2(0.01)),\n",
        "    Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "model_no_dropout.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history_no_dropout = model_no_dropout.fit(\n",
        "    X_train_pad, y_train,\n",
        "    epochs=20,\n",
        "    batch_size=32,\n",
        "    validation_data=(X_val_pad, y_val),\n",
        "    class_weight=weights_dict,\n",
        "    callbacks=[lr_scheduler, early_stopping],\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "# Évaluer sur test\n",
        "y_pred_no_dropout = np.argmax(model_no_dropout.predict(X_test_pad), axis=1)\n",
        "print(\"Test accuracy sans Dropout :\", accuracy_score(y_test, y_pred_no_dropout))\n"
      ],
      "metadata": {
        "id": "ZBuub911UZrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_no_l2 = Sequential([\n",
        "    Embedding(max_words, embedding_dim, input_length=max_len),\n",
        "    LSTM(128, return_sequences=False),\n",
        "    Dropout(0.5),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "model_no_l2.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history_no_l2 = model_no_l2.fit(\n",
        "    X_train_pad, y_train,\n",
        "    epochs=20,\n",
        "    batch_size=32,\n",
        "    validation_data=(X_val_pad, y_val),\n",
        "    class_weight=weights_dict,\n",
        "    callbacks=[lr_scheduler, early_stopping],\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "# Évaluer sur test\n",
        "y_pred_no_l2 = np.argmax(model_no_l2.predict(X_test_pad), axis=1)\n",
        "print(\"Test accuracy sans L2 :\", accuracy_score(y_test, y_pred_no_l2))\n"
      ],
      "metadata": {
        "id": "Cf6t4NxnUcPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_no_weight = Sequential([\n",
        "    Embedding(max_words, embedding_dim, input_length=max_len),\n",
        "    LSTM(128, return_sequences=False, kernel_regularizer=l2(0.01)),\n",
        "    Dropout(0.5),\n",
        "    Dense(64, activation='relu', kernel_regularizer=l2(0.01)),\n",
        "    Dropout(0.5),\n",
        "    Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "model_no_weight.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history_no_weight = model_no_weight.fit(\n",
        "    X_train_pad, y_train,\n",
        "    epochs=20,\n",
        "    batch_size=32,\n",
        "    validation_data=(X_val_pad, y_val),\n",
        "    callbacks=[lr_scheduler, early_stopping],  # sans class_weight\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "# Évaluer sur test\n",
        "y_pred_no_weight = np.argmax(model_no_weight.predict(X_test_pad), axis=1)\n",
        "print(\"Test accuracy sans class_weight :\", accuracy_score(y_test, y_pred_no_weight))\n"
      ],
      "metadata": {
        "id": "Y-mGi7CgUd5b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}